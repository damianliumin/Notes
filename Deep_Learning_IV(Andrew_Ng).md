# Convolutional Neural Networks

Author: Daniel Liu
Contact me:  191240030@smail.nju.edu.cn
Course: Deeplearning.ai Part IV - AndrewNg

## L1 - 卷积神经网络 Convolutional Neural Networks

### 1 计算机视觉

在深度学习的影响下，计算机视觉取得了飞速的发展并应用于许多领域。CV中经常会诞生一些非常新奇的idea，从而启发其他领域的研究工作。目前常见的CV问题包括**图像分类**、**目标检测(在图片中定位到目标)**、**风格迁移**等。在CV中，输入的特征数非常大，用全连接的NN会带来非常庞大的计算量，而CNN可以很好地适应CV的任务。

### 2 卷积

**边缘检测：**
为了检测图像的边缘，可以使用一个$3\times 3$的**滤波器 filter** (也称**卷积核 kernel**)与原图像进行卷积运算。过滤器会不断覆盖图像$3\times 3$的位置，将图像该区域和滤波器进行元素相乘，将计算结果相加置入一个新矩阵的对应位置。例如：
$$
\left[ \begin{matrix} 10 & 10 & 10 & 0 & 0 & 0 \\ 10 & 10 & 10 & 0 & 0 & 0 \\ 10 & 10 & 10 & 0 & 0 & 0 \\ 10 & 10 & 10 & 0 & 0 & 0 \\ 10 & 10 & 10 & 0 & 0 & 0 \\ 10 & 10 & 10 & 0 & 0 & 0 \\ \end{matrix} \right] * \left[ \begin{matrix} 1 & 0 & -1 \\ 1 & 0 & -1 \\ 1 & 0 & -1  \end{matrix} \right] = \left[ \begin{matrix} 0 & 30 & 30 & 0\\ 0 & 30 & 30 & 0\\  0 & 30 & 30 & 0\\  0 & 30 & 30 & 0\end{matrix} \right]
$$
左边是一个$6\times 6$的灰度图，中间是$3\times 3$的滤波器用于垂直边缘检测，通过卷积运算$*$得到$4\times 4$的结果。卷积运算检测出了0和10的边界。如果原图中0和10位置互换，得到的结果会是-30，如果不在意30和-30的区别可以加上绝对值。将$3\times 3$的滤波器旋转$90^\circ$即可得到水平边缘检测器。在边缘检测问题上，还有一些鲁棒性更强、特性不同的滤波器，例如：
$$
\textbf{sobel filter: } \left[ \begin{matrix} 1 & 0 & -1 \\ 2 & 0 & -2 \\ 1 & 0 & -1  \end{matrix} \right]\\
\textbf{schorr filter: } \left[ \begin{matrix} 3 & 0 & -3 \\ 10 & 0 & -10 \\ 3 & 0 & -3  \end{matrix} \right]
$$
现在计算机视觉中，通常将滤波器作为参数：$\left[ \begin{matrix} w_1 & w_2 & w_3 \\ w_4 & w_5 & w_6 \\ w_7 & w_8 & w_9 \end{matrix} \right]$，通过反向传播算法得到更为巧妙的滤波器。编程语言和框架中，python的conv_forward，tensorflow的tf.nn.conv2d和Keras的Conv2D都可用于卷积运算。

**Padding:**
使用卷积的过程中，如果不考虑Padding会发现图片越来越小，且边缘的像素被卷积覆盖的次数很少，从而导致信息的利用率不高。为了解决这一问题，通常会在图像边缘填充一些像素(常设置为0)，从而保证输入和输出的图像大小相同。一般Padding策略有以下两种：

+ Valid：不填充边缘，$n\times n$的图像与$f\times f$的滤波器卷积运算后得到$(n-f+1)\times (n-f+1)$的输出
+ Same：填充边缘保证输出和输入大小相同，边缘填充$p=\frac{f-1}{2}$

按照惯例，CV中滤波器的$f$选择$1,3,5,7$这样的奇数，这部分是因为奇数能确保Padding策略可以完好计算出$p$，同时奇数保证了滤波器有一个中心。

**卷积步长：**
滤波器在原图像上移动时，定义移动的距离**步长 stride**，记为$s$。在此基础上，$n\times n$的图像与$f\times f$的滤波器，边缘填充$p$，得到的输出大小为$\lfloor \frac{n+2p-f}{s} + 1 \rfloor \times \lfloor \frac{n+2p-f}{s} + 1 \rfloor$.

### 3 三维卷积

若输入的图像有RGB三个颜色通道，而非单一的灰度图，这时输入的维度变为$n\times n\times n_c$。为了应对这种输入，滤波器也会变为$f\times f \times n_c$，每个颜色通道的图像和滤波器进行卷积运算后结果相加，存放到$(n-f+1)\times (n-f+1)$的输出当中(暂不考虑Padding和步长)。注意输出只有2维。

**多个滤波器：**
有时我们希望同时检测图像各种角度的边缘，这种情况下可以采用多个滤波器，分别与输入进行卷积运算。这样维度变为$(n\times n\times n_c)*(f\times f \times n_c)\rightarrow ((n-f+1)\times (n-f+1)\times n_c')$，其中$n_c'$为滤波器个数。

### 4 卷积层

在通过卷积运算得到一个$(n-f+1)\times (n-f+1)\times n_c'$矩阵的基础上，对其中每个$(n-f+1)\times (n-f+1)$的矩阵分别加上一个偏置$b$，然后对其使用非线性激活函数，就可以得到一个单层卷积神经网络。可以发现，CNN每层参数数量仅取决于滤波器的大小和数量，与输入大小无关，因而CNN能够用相对较少的参数处理很大规模的输入。

**Notation:**
$f^{[l]}$: 第$l$层滤波器大小
$p^{[l]}$: padding策略，same或valid
$s^{[l]}$: 步长
$n_c^{[l]}$: 滤波器数量
**维度：**
每个滤波器: $f^{[l]}\times f^{[l]} \times n_c^{[l-1]}$
每层激活函数输出: $n_H^{[l]}\times n_W^{[l]}\times n_c^{[l]}$
权重矩阵: $f^{[l]}\times f^{[l]} \times n_c^{[l-1]}\times n_c^{[l]}$
偏置: $1\times 1\times 1\times n_c^{[l]}$
输入: $n_H^{[l-1]}\times n_W^{[l-1]}\times n_c^{[l-1]}$
输出: $n_H^{[l]}\times n_W^{[l]}\times n_c^{[l]}$
其中$n_H^{[l]}=\lfloor \frac{n_H^{[l]}+2p^{[l]}-f^{[l]}}{s^{[l]}} + 1\rfloor, n_W^{[l]}=\lfloor \frac{n_W^{[l]}+2p^{[l]}-f^{[l]}}{s^{[l]}} + 1\rfloor$.

通常一个卷积神经网络经过多个卷积层后会得到一个$n_H\times n_W\times n_c$的输出，包含从图像中提取出的大量特征。我们将该输出展开，并用Softmax等层作为输出，从而解决分类问题等。这样我们就得到了一个简单的卷积神经网络。显然CNN中有大量超参数要选取，大多数网络会在前几层保持输入的size，然后逐渐减小，并逐渐增加channel数量。

另外，CNN中通常包含三种层：

+ 卷积层 Convolution (CONV)
+ 池化层 Pooling (POOL)
+ 全连接层 Fully Connected (FC)

大多数CNN架构都会包含这些层。

### 5 池化层

池化层可以**减小模型规模**、提高计算速度，并提高所提取特征的鲁棒性。它将一个过滤器覆盖在输入上，如果使用Max Pooling，在覆盖区域内选出最大的元素放入输出的对应位置内，若是Average Pooling，则将平均值放入输出对应位置。对于多信道的输入，则对每个信道分别进行如上操作。在池化层中**没有任何需要学习的参数**，只需要选取超参数$f,s$ (池化层一般不会使用Padding)，$f=s=2$的池化层会将输入的大小缩减为原来的一半。因此，池化层的输入到输出维度变化为：$n_H\times n_W\times n_c\rightarrow \lfloor \frac{n_H-f}{s}+1 \rfloor \times \lfloor \frac{n_W-f}{s}+1 \rfloor\times n_c$.

**Max Pooling直观理解：**目前最常用且效果最好的池化方法是Max Pooling，对它的直观理解是：如果一个区域内最大值比较大，则该区域很可能提取到了某个重要特征，反之可能该区域没有这个特征，将最大值输出可以很好地反映局部的特征。

通常情况下，统计CNN中层数时仅考虑有参数的层，因而池化层不在考虑范围内。很多文献中会把一组(CONV, POOL)作为一层。

### 6 卷积神经网络

一种常见的卷积神经网络的的模式是：
CONV - POOL - CONV - POOL - FC -FC - FC - Softmax

卷积层和池化层的部分参数较少，通常随着层数的增加，激活函数输出的$n_W,n_H$会减小，信道数会增加。全连接层的参数较多，采用的是经典的NN结构。每层激活函数输出的规模一般会逐渐减小，过快减小会影响网络的性能。

CNN有两个重要的特征：

+ 参数共享 Parameter sharing: 一个特征检测器如果在图像的某一部分适用，那么也很可能在图像的其他部分适用
+ 稀疏连接 Sparsity of connections: 在每一层，输出的每一个值都只与输入的$f^{[l]2}\cdot n_c^{[l-1]}$个值连接。

这两个特性使得CNN的参数较少，可以在特征数量庞大的CV问题中有效防止过拟合。另外，它们也使得CNN具有很好的**平移不变性 translation invariance**，即识别目标在图像中的移动不会对识别结果产生多大影响。

***

## L2 - 深度卷积网络：实例研究 Case Studies

### 1 经典网络

**1.1 LeNet-5**
LeNet-5上世纪90年代就已提出，大约6万个参数。

**1.2 AlexNet**
AlexNet结构与LeNet-5相似，但是规模更大，有大约6000万个参数。另一方面，提出AlexNet时，ReLU也被广泛用于激活函数，这使得AlexNet性能更好。

**1.3 VGG-16**
VGG-16规模很大，有大约1.38亿个参数，但是它结构规整简单。16表示卷积层和全连接层的总数，另有VGG-19，但是由于两者表现相似，一般还是选择VGG-16。

### 2 残差网络

理论上越深的网络会有越好的性能，但是由于梯度消失和梯度爆炸等原因，层数很深的网络往往很难训练，达到某一层数后性能甚至会随着层数的增加而降低。为了解决这一问题，**残差网络 ResNet (Residual Network)**在**常规网络Plain network**数值传播的**主路径**外添加了**远跳连接 Short cut (Skip Connection)**，使得深层的网络能够展现出接近理论的性能。

**残差块 Residual Block：**
在常规网络中，$a^{[l]}$到$a^{[l+2]}$的计算过程如下：
$z^{[l+1]}=W^{[l+1]}a^{[l]}+b^{[l+1]}\\ a^{[l+1]}=g(z^{[l+1]})\\z^{[l+2]}=W^{[l+2]}a^{[l+1]}+b^{[l+2]}\\ a^{[l+2]}=g(z^{[l+2]})$
现在我们将计算$a^{[l+2]}$的方法改为$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$，这里$a^{[l]},a^{[l+1]},a^{[l+2]}$组成了一个残差块。
将多个残差块串联起来，也即每两层组成一个残差块，即可得到一个残差网络。

**残差网络原理理解：**
显然，残差网络使得层数可以变得更深，从而带来更好的模型。但是我们需要理解为什么残差网络能够提升深层网络的训练效率。考虑残差块：$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})=g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})$，如果$W^{[l+2]}a^{[l+1]}+b^{[l+2]}=0$且$g$为ReLU函数，可以得到$a^{[l+2]}=a^{[l]}$，这种恒等式在残差网络中很容易得到，因而前层的数值可以更加快捷地传递到深层。因此，残差网络训练效率更高。
这里有一个需要注意的细节，通常残差网络中会有大量输入输出大小相等的卷积层保证$z^{[l+2]}$和$a^{[l]}$维度相同，如果这两者维度不同，可以通过添加可学习参数计算$Wa^{[l]}$以获得相同的维度。

### 3 Inception网络

$1\times1$**卷积：**
$1\times1$卷积，也称network in network，可调整信道的数量。对于每个$1\times 1$滤波器，它与输入每个信道对应位置全连接，维度变化为$n\times n \times n_c^{[l-1]}\rightarrow n\times n\times n_c^{[l]}$。与此同时，非线性的激活函数使得网络可以得到更为复杂的函数。

**Inception网络：**
选择什么数量、大小的滤波器以及选择卷积层还是池化层往往是比较麻烦的问题，Inception网络将不同的结构组合起来，让网络自己学习，从而省去了上述麻烦。例如，对于$28\times 28\times 192$的输入，Inception网络选取含有$1\times 1, 3\times 3, 5\times 5$滤波器的卷积层和MAX-POOL的池化层，将其输出堆叠称一个$28\times 28\times 192$的网络(为了维数相同，这里的池化层需要用Same Padding处理)。

**计算成本：**
显然Inception的计算成本很高，为了解决这一问题，可以在上述两层之间添加一个bottle-neck层：通过$1\times 1$卷积将$28\times 28\times 192$的输入压缩成$28\times 28\times 16$，将后者作为下一层的输入。合理设计的瓶颈结构可以大幅减小计算规模，同时保持网络的性能。

### 4 迁移学习

对于很多深度学习任务，可以从网络上下载开源的已经训练好的模型，然后根据自己的数据对其重新训练即可。在数据量较小的时候，可以仅仅将softmax层改为自己需要的，冻结前面所有的层并训练。在数据量较多的时候，可以重新训练网络的最后几层。如果数据量很庞大，甚至可以直接将下载的权重作为网络的初始值，重新训练整个网络。

***















